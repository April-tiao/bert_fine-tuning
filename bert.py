import os
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns  

from transformers import TrainingArguments

from datasets import Dataset
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)

# ========== Step 1. æ•°æ®åŠ è½½ ==========
df = pd.read_csv(r"D:\projects\BERT\å¸®åŠ©ä¿¡æ¯ç½‘ç»œå®‰å…¨çŠ¯ç½ª2_new.csv")

# ========== Step 2. ç¼ºå¤±å€¼å¤„ç† ==========
df["æ¡ˆç”±"] = df["æ¡ˆç”±"].fillna("æœªçŸ¥")
df["è£åˆ¤ä¾æ®"] = df["è£åˆ¤ä¾æ®"].fillna("ç¼ºå¤±")
df["ç±»å‹"] = df["ç±»å‹"].fillna("æœªçŸ¥")
df["åœ°å€"] = df["åœ°å€"].fillna("æœªçŸ¥")
df["æ—¶é—´"] = pd.to_datetime(df["æ—¶é—´"], errors="coerce")
df = df.dropna(subset=["ç½šé‡‘"])

# ========== Step 3. åˆ‘æœŸè½¬æ¢ï¼ˆæœˆï¼‰ ==========
def extract_months(term):
    if pd.isna(term):
        return None
    y = re.search(r"(\d+)å¹´", term)
    m = re.search(r"(\d+)ä¸ªæœˆ", term)
    return (int(y.group(1)) * 12 if y else 0) + (int(m.group(1)) if m else 0)

df["åˆ‘æœŸï¼ˆæœˆï¼‰"] = df["åˆ‘æœŸ"].apply(extract_months)

# ========== Step 4. æ ‡ç­¾åˆ†ç±» ==========
def classify_term(months):
    if months is None:
        return None
    if months <= 12:
        return 0
    elif months <= 36:
        return 1
    return 2

df["label"] = df["åˆ‘æœŸï¼ˆæœˆï¼‰"].apply(classify_term)

# ========== Step 5. æ–‡æœ¬æ‹¼æ¥ ==========
df["text"] = df["æ¡ˆç”±"].fillna('') + "ã€‚" + df["è£åˆ¤ç»“æœ"].fillna('')
df = df.dropna(subset=["text", "label"])

# ========== Step 6. åˆ’åˆ†æ•°æ®é›† ==========
train_df, test_df = train_test_split(
    df[["text", "label"]],
    test_size=0.2,
    stratify=df["label"],
    random_state=42
)

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# ========== Step 7. åˆ†è¯å™¨ ==========
model_name = "bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model_name)

def tokenize_function(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=512)

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_test = test_dataset.map(tokenize_function, batched=True)

# ========== Step 8. è‡ªå®šä¹‰ FocalLoss ==========
class FocalLoss(nn.Module):
    def __init__(self, alpha=1.0, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, target):
        ce_loss = F.cross_entropy(logits, target, reduction='none')
        pt = torch.exp(-ce_loss)
        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return loss.mean() if self.reduction == "mean" else loss.sum()

# ========== Step 9. è‡ªå®šä¹‰ Trainer ==========
class FocalLossTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss = FocalLoss()(logits, labels)
        return (loss, outputs) if return_outputs else loss

# ========== Step 10. æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ==========
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)

# training_args = TrainingArguments(
#     output_dir="./results",
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     gradient_accumulation_steps=1,
#     num_train_epochs=5,
#     learning_rate=2e-5,
#     weight_decay=0.01,
#     # evaluation_strategy="epoch",
#     eval_strategy="epoch",
#     save_strategy="epoch",
#     logging_strategy="epoch",
#     lr_scheduler_type="linear",
#     warmup_steps=200,
#     load_best_model_at_end=True,
#     metric_for_best_model="f1",
#     save_total_limit=2,
#     fp16=True,  # éœ€è¦å®‰è£… NVIDIA apex æˆ– PyTorch AMP æ”¯æŒ
#     report_to="none"  # å…³é—­ wandb ç­‰è¿œç¨‹æ—¥å¿—
# )
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=1,
    num_train_epochs=20,
    learning_rate=2e-5,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    lr_scheduler_type="cosine",  # æ›´å¹³æ»‘
    warmup_steps=50,  # æ•°æ®é‡å°ï¼Œå¯å‡å°‘ warmup
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    save_total_limit=2,
    fp16=True,
    report_to="none"
)


def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted")
    }

trainer = FocalLossTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# ========== Step 11. æ¨¡å‹è®­ç»ƒ ==========
train_result = trainer.train()
trainer.save_model()


# ========== Step 12. ç»˜åˆ¶è®­ç»ƒä¸éªŒè¯ Loss æ›²çº¿ ==========
logs = trainer.state.log_history
losses = [log["loss"] for log in logs if "loss" in log]
eval_losses = [log["eval_loss"] for log in logs if "eval_loss" in log]
steps = [log["step"] for log in logs if "loss" in log]

plt.figure(figsize=(8, 6))
plt.plot(steps, losses, label="Train Loss", color="royalblue", linewidth=2)
if eval_losses:
    eval_steps = [log["step"] for log in logs if "eval_loss" in log]
    plt.plot(eval_steps, eval_losses, label="Eval Loss", color="darkorange", linewidth=2)

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# ========== Step 13. æ¨¡å‹è¯„ä¼° ==========
predictions = trainer.predict(tokenized_test)
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=1)

print("\nğŸ“Š Classification Report:")
print(classification_report(y_true, y_pred, digits=4))

# ========== Step 13. ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ ==========
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6, 5))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap="Blues",      # è¶Šæ·±é¢œè‰²ä»£è¡¨æ ·æœ¬æ•°è¶Šå¤š
    cbar=True,
    square=True,
    linewidths=0.5,
    linecolor="gray"
)
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.title("Confusion Matrix (Focal Loss + BERT)")
plt.tight_layout()
plt.show()

# ========== Step 14. ä¿å­˜é¢„å¤„ç†æ•°æ® ==========
os.makedirs("data", exist_ok=True)
df.to_excel("data/processed_dataset.xlsx", index=False)
print("âœ… å·²ä¿å­˜æ•°æ®è‡³ ./data/processed_dataset.xlsx")
print("âœ… æ ‡ç­¾åˆ†å¸ƒ:", Counter(df["label"]))
