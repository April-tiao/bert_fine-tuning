
import os, re, math
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset as TorchDataset

import matplotlib.pyplot as plt
from matplotlib import font_manager
from sklearn.decomposition import PCA
from transformers import BertTokenizer, BertModel

# ---------------- å…¨å±€è®¾ç½® ----------------
plt.rcParams["font.family"] = "SimHei"          # ä¸­æ–‡
plt.rcParams["axes.unicode_minus"] = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥  running on: {device}")

# ---------------- è¯»å– & é¢„å¤„ç† ----------------
df = pd.read_csv(r"D:\projects\BERT\å¸®åŠ©ä¿¡æ¯ç½‘ç»œå®‰å…¨çŠ¯ç½ª2_new.csv")

# ç¼ºå¤±å€¼å¤„ç†
df["æ¡ˆç”±"]   = df["æ¡ˆç”±"].fillna("æœªçŸ¥")
df["è£åˆ¤ä¾æ®"] = df["è£åˆ¤ä¾æ®"].fillna("ç¼ºå¤±")
df["ç±»å‹"]   = df["ç±»å‹"].fillna("æœªçŸ¥")
df["åœ°å€"]   = df["åœ°å€"].fillna("æœªçŸ¥")
df["æ—¶é—´"]   = pd.to_datetime(df["æ—¶é—´"], errors="coerce")
df          = df.dropna(subset=["ç½šé‡‘"])

# åˆ‘æœŸï¼ˆæœˆï¼‰
def extract_months(term):
    if pd.isna(term):
        return np.nan
    y = re.search(r"(\d+)å¹´", term)
    m = re.search(r"(\d+)ä¸ªæœˆ", term)
    return (int(y.group(1)) * 12 if y else 0) + (int(m.group(1)) if m else 0)

df["åˆ‘æœŸï¼ˆæœˆï¼‰"] = df["åˆ‘æœŸ"].apply(extract_months)
df["log_ç½šé‡‘"]   = np.log1p(df["ç½šé‡‘"])         # é•¿å°¾ â†’ å¯¹æ•°
df = df.dropna(subset=["åˆ‘æœŸï¼ˆæœˆï¼‰"])            # å°‘é‡ç¼ºå¤±å¯ç›´æ¥å‰”é™¤

# æ‹¼æ¥æ–‡æœ¬
df["text"] = df["æ¡ˆç”±"].fillna("") + "ã€‚" + df["è£åˆ¤ç»“æœ"].fillna("")

# ---------------- æ–‡æœ¬ BERT å‘é‡ ----------------
model_name = "bert-base-chinese"
tokenizer   = BertTokenizer.from_pretrained(model_name)
bert_model  = BertModel.from_pretrained(model_name).to(device).eval()

class TextDataset(TorchDataset):
    def __init__(self, texts):
        self.texts = texts
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        return self.texts[idx]

def collate_fn(batch):
    enc = tokenizer(
        batch,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    return {k: v.to(device) for k, v in enc.items()}

loader = DataLoader(
    TextDataset(df["text"].tolist()),
    batch_size=32,              # â†” GPU æ˜¾å­˜
    shuffle=False,
    collate_fn=collate_fn
)

embeddings = []
with torch.no_grad():
    for batch in tqdm(loader, desc="BERT embedding"):
        out = bert_model(**batch).last_hidden_state[:, 0, :]   # CLS
        embeddings.append(out.cpu())
embeddings = torch.cat(embeddings).numpy()                    # (N, 768)

emb_cols = [f"emb_{i}" for i in range(embeddings.shape[1])]
df[emb_cols] = pd.DataFrame(embeddings, index=df.index)

# ---------------- Spearman ç›¸å…³ ----------------
numeric_cols = ["åˆ‘æœŸï¼ˆæœˆï¼‰", "ç½šé‡‘", "log_ç½šé‡‘"]

# 1) 3 Ã— 768 å­çŸ©é˜µ
corr_large = (
    df[numeric_cols + emb_cols]
    .corr(method="spearman")
    .loc[numeric_cols, emb_cols]
    .abs()
)

# 2) PCA å‰ 2 ç»´ â†’ 5Ã—5 å°çŸ©é˜µ
pca = PCA(n_components=2, random_state=42)
pca_vec = pca.fit_transform(embeddings)          # (N, 2)
df["emb_pca1"] = pca_vec[:, 0]
df["emb_pca2"] = pca_vec[:, 1]
small_cols = numeric_cols + ["emb_pca1", "emb_pca2"]
corr_small = df[small_cols].corr(method="spearman")

# ---------------- ç»˜å›¾ ----------------
def plot_heat(data, xticks, yticks, title, figsize):
    fig, ax = plt.subplots(figsize=figsize)
    im = ax.imshow(data, cmap="Blues", vmin=0, vmax=1, aspect="auto")
    ax.set_xticks(range(len(xticks)))
    ax.set_xticklabels(xticks, rotation=45, ha="right")
    ax.set_yticks(range(len(yticks)))
    ax.set_yticklabels(yticks)
    # æ•°å€¼æ ‡æ³¨ï¼ˆå°å›¾æ‰å†™ï¼‰
    if data.shape[0] <= 10 and data.shape[1] <= 10:
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                ax.text(j, i, f"{data[i, j]:.2f}", ha="center", va="center")
    fig.colorbar(im, ax=ax, label="|Spearman Ï|")
    ax.set_title(title)
    plt.tight_layout()
    plt.show()

plot_heat(
    corr_large.values,
    xticks=[],                         # 768 ç»´å¤ªå¤š
    yticks=numeric_cols,
    title="Numeric â†” BERT-CLS ç›¸å…³ (|Ï|)",
    figsize=(28, 4)
)

plot_heat(
    corr_small.abs().values,
    xticks=small_cols,
    yticks=small_cols,
    title="Spearman ç›¸å…³çŸ©é˜µ (å« BERT-PCA)",
    figsize=(6, 5)
)
