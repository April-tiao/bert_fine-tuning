# kmeans_cluster.py
# -----------------------------------------------
# èšç±»åˆ‘æœŸï¼ˆæœˆï¼‰+ log_ç½šé‡‘ + BERT-PCA è¯­ä¹‰ä¸»æˆåˆ†
# -----------------------------------------------
import os, re
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

import torch
from torch.utils.data import DataLoader, Dataset as TorchDataset
from transformers import BertTokenizer, BertModel

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams["font.family"] = "SimHei"
plt.rcParams["axes.unicode_minus"] = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥  running on: {device}")

# ---------- 1. æ•°æ®è¯»å– ----------
CSV_PATH = r"D:\projects\BERT\å¸®åŠ©ä¿¡æ¯ç½‘ç»œå®‰å…¨çŠ¯ç½ª2_new.csv"  # â† æ”¹æˆè‡ªå·±çš„è·¯å¾„
df = pd.read_csv(CSV_PATH)

# ---------- 2. å­—æ®µé¢„å¤„ç† ----------
def extract_months(term: str) -> float:
    if pd.isna(term):
        return np.nan
    y = re.search(r"(\d+)å¹´", term)
    m = re.search(r"(\d+)ä¸ªæœˆ", term)
    return (int(y.group(1))*12 if y else 0) + (int(m.group(1)) if m else 0)

df["åˆ‘æœŸï¼ˆæœˆï¼‰"] = df["åˆ‘æœŸ"].apply(extract_months)
df["log_ç½šé‡‘"]   = np.log1p(df["ç½šé‡‘"])
df.dropna(subset=["åˆ‘æœŸï¼ˆæœˆï¼‰", "log_ç½šé‡‘"], inplace=True)

df["æ¡ˆç”±"]   = df["æ¡ˆç”±"].fillna("æœªçŸ¥")
df["è£åˆ¤ç»“æœ"] = df["è£åˆ¤ç»“æœ"].fillna("")
df["text"]   = df["æ¡ˆç”±"] + "ã€‚" + df["è£åˆ¤ç»“æœ"]

# ---------- 3. BERT CLS å‘é‡ ----------
class TextDS(TorchDataset):
    def __init__(self, texts): self.texts = texts
    def __len__(self): return len(self.texts)
    def __getitem__(self, i): return self.texts[i]

model_name = "bert-base-chinese"
tokenizer   = BertTokenizer.from_pretrained(model_name)
bert_model  = BertModel.from_pretrained(model_name).to(device).eval()

def collate_fn(batch):
    enc = tokenizer(batch, padding=True, truncation=True,
                    max_length=512, return_tensors="pt")
    return {k: v.to(device) for k, v in enc.items()}

loader = DataLoader(TextDS(df["text"].tolist()),
                    batch_size=32, shuffle=False, collate_fn=collate_fn)

embeddings = []
with torch.no_grad():
    for batch in tqdm(loader, desc="BERT embedding"):
        out = bert_model(**batch).last_hidden_state[:, 0, :]   # CLS
        embeddings.append(out.cpu())
embeddings = torch.cat(embeddings).numpy()                    # (N, 768)

# ---------- 4. PCA â†’ 2 ç»´ ----------
pca = PCA(n_components=2, random_state=42)
pca_vec = pca.fit_transform(embeddings)                        # (N, 2)
df["emb_pca1"] = pca_vec[:, 0]
df["emb_pca2"] = pca_vec[:, 1]

# ---------- 5. èšç±»å‡†å¤‡ ----------
feature_cols = ["åˆ‘æœŸï¼ˆæœˆï¼‰", "log_ç½šé‡‘", "emb_pca1", "emb_pca2"]
X = df[feature_cols].values
X = StandardScaler().fit_transform(X)                          # æ ‡å‡†åŒ–

# ---------- 6. ç”Ÿæˆè¿‡ç¨‹è¡¨æ ¼ ----------
process_table = pd.DataFrame({
    "æ­¥éª¤": [1, 2, 3, 4, 5],
    "å…³é”®æ“ä½œ": ["ç‰¹å¾é€‰å–", "æ ‡å‡†åŒ–", "K-means è®­ç»ƒ", "è¯„ä¼° K", "ç»“æœè¾“å‡º"],
    "ç»†èŠ‚": [", ".join(feature_cols),
             "StandardScaler å‡å€¼0æ–¹å·®1",
             "KMeans(n_init='auto', random_state=42)",
             "inertia_ / silhouette_score",
             "Excel+æ•£ç‚¹å›¾"]
})
print("\nğŸ—’  è¿‡ç¨‹è¡¨æ ¼")
print(process_table.to_markdown(index=False))

# ---------- 7. K-means (k=3,4) ----------
k_list = [3, 4]
summary_tables = {}

for k in k_list:
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = km.fit_predict(X)
    df[f"cluster{k}"] = labels
    sil = silhouette_score(X, labels)
    print(f"âœ…  k={k}  inertia={km.inertia_:.1f}  silhouette={sil:.3f}")

    # æ±‡æ€»ç»Ÿè®¡
    tbl = (df.groupby(f"cluster{k}")[feature_cols]
           .agg(["count", "mean", "std"])
           .round(2))
    summary_tables[k] = tbl
    print(f"\nğŸ“Š  k={k} ç»“æœè¡¨æ ¼")
    print(tbl.to_markdown())

    # æ•£ç‚¹å›¾
    plt.figure(figsize=(6,5))
    sns.scatterplot(x="emb_pca1", y="emb_pca2",
                    hue=f"cluster{k}",
                    palette=sns.color_palette("Set2", k),
                    data=df, s=40, edgecolor="k")
    plt.title(f"K-means èšç±» (k={k}) in PCA Space")
    plt.xlabel("emb_pca1")
    plt.ylabel("emb_pca2")
    plt.legend(title="cluster", loc="best")
    plt.tight_layout()
    plt.savefig(f"cluster_scatter_k{k}.png", dpi=300)
    plt.show()

# ---------- 8. ä¿å­˜ Excel ----------
os.makedirs("cluster_output", exist_ok=True)
with pd.ExcelWriter("cluster_output/cluster_summary.xlsx") as writer:
    process_table.to_excel(writer, sheet_name="process", index=False)
    for k, tbl in summary_tables.items():
        tbl.to_excel(writer, sheet_name=f"k{k}")

print("\nğŸ‰  å…¨éƒ¨å®Œæˆï¼å·²ç”Ÿæˆï¼š")
print("    Â· cluster_output/cluster_summary.xlsx")
print("    Â· cluster_scatter_k3.png / cluster_scatter_k4.png")
